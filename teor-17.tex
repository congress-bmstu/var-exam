\que{Сильный и слабый дифференциалы функционала, связь между ними (с доказательством). Единственность сильного и слабого дифференциала (с доказательством).}

Рассмотрим приращение функционала $J (y)$:
\[
  \Delta J = J(y + h) - J(y).
\]

\begin{definition}
  \emph{Сильным дифференциалом} функционала $J$ называется линейный функционал $dJ$ такой, что
  \[
    \Delta J = dJ (h) + \alpha \|h\|_0,
  \]
  где $\alpha(h) \to 0$, при $\|h\|_0 \to 0$.
\end{definition}

\begin{definition}
  \emph{Слабым дифференциалом} функционала $J$ называется линейный функционал $\delta J$ такой, что
  \[
    \Delta J = \delta J (h) + \alpha \|h\|_1,
  \]
  где $\alpha(h) \to 0$, при $\|h\|_1 \to 0$.
\end{definition}

\begin{theorem}[связь между слабыми и сильными дифференциалами]
  Если $dJ$ -- сильный дифференциал, тогда $dJ$ также является и слабым дифференциалом.
\end{theorem}
\begin{proof}
  Пусть $dJ$ -- сильный дифференциал, тогда 
  \[
    \Delta J = dJ(h) + \alpha \cdot \|h\|_0,
  \]
  где $\alpha(h) \to 0$, при $\|h\|_0 \to 0$.

  Для того, чтобы $dJ$ был слабым дифференциалом, необходимо, чтобы 
  \[
    \Delta J = dJ(h) + \beta \cdot \|h\|_1,
  \]
  где $\beta \to 0$, при $\|h\|_1 \to 0$.

  Пусть теперь $\|h\|_1 = \max |h(x)| + \max |h'(x)| \to 0$, тогда $\|h\|_0 = \max |h(x)| \to 0$.
  Если выбрать $\beta = \alpha \cdot \dfrac{\|h\|_0}{\|h\|_1}$, то всё выполняется, ибо
  $\alpha \to 0$, а
  $\dfrac{\|h\|_0}{\|h\|_1} = \dfrac{\max |h(x)|}{\max |h(x)| + \max |h'(x)|} < 1$ -- ограниченное.
\end{proof}


\begin{theorem}[единственность сильного и слабого дифференциала]
  Сильный дифференциал функционала определён однозначно.
\end{theorem}
\begin{proof}
  \begin{enumerate}
    \item Докажем, что если $\varphi(h)$ -- линейный функционал и
      $\dfrac{\varphi(h)}{\|h\|_0} \to 0$ при $\|h\|_0 \to 0$,
      то $\varphi(h) \equiv 0$. Действительно, докажем от противного: пусть
      $\varphi(h_0) = \lambda \neq 0$. Заметим, что $h_0 \neq 0$, т.к. линейное отображение
      переводит 0 в 0. Рассмотрим последовательность $h_n = \dfrac{h_0}{n}, n \in \mathbb{N}$.
      Тогда:
      \[
        \| h_n \|_0 = \|\dfrac{h_0}{n}\|_0 = \dfrac{1}{n} \cdot \|h_0\|_0 = \dfrac{\| h_0 \|_0}{n},
      \]
      тогда по условию: 
      \[
        \lim_{n\to\infty} \dfrac{\varphi(h_0)}{\|h_n\|} = 0,
      \]
      т.к. из сходимости по Коши следует сходимость по Гейне. Но с другой стороны:
      \[
        \varphi(h_n) - \varphi( \dfrac{h_0}{n} ) = \dfrac{1}{n} \varphi(h_0) = \dfrac{\lambda}{n},
      \]
      \[
        \lim_{n\to\infty} \dfrac{\varphi(h_n)}{\|h_n\|_0} =
        \lim_{n\to\infty} \dfrac{ \dfrac{\lambda}{n} }{ \dfrac{\|h_0\|_0}{n} } =
        \lim_{n\to\infty} \dfrac{\lambda}{\|h_0\|_0} =
        \dfrac{\lambda}{\|h_0\|_0} \neq 0,
      \]
      т.к. $\lambda \neq 0$ и $\|h_0\|_0 \neq 0$, т.к. $h_0 \neq 0$.

    \item Докажем от противного: пусть $dJ_1$ и $dJ_2$ -- сильные дифференциалы функционала $J$:
      \[
        \begin{cases}
          \Delta J = dJ_1(h) + \alpha_1 \cdot \|h\|_0, \\
          \Delta J = dJ_2(h) + \alpha_2 \cdot \|h\|_0, \\
        \end{cases}
      \]
      где $\alpha_1, \alpha_2 \to 0$, при $\|h\|_0 \to 0$.
      Приравняем:
      \[
        dJ_1 (h) - dJ_2 (h) = (\alpha_2 - \alpha_1) \cdot \|h\|_0,
      \]
      где $\alpha_2-\alpha_1 \to 0$ при $\|h\|_0$,
      тогда:
      \[
        \dfrac{dJ_1 - dJ_2}{\|h\|_0} = \dfrac{\alpha_2 - \alpha_1}{\|h\|_0} \cdot \|h\|_0 \to 0,
      \]
      при $\|h\|_0 \to 0$. Тогда т.к. $dJ_1 - dJ_2$ -- линейный функционал, то
      $dJ_1 - dJ_2 = 0$.
  \end{enumerate}
\end{proof}
